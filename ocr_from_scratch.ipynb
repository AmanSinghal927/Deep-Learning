{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "In this assignment we're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task.\n",
        "The purpose of this homework is for you to get familar with ViT and get prepared for the final project. "
      ],
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tB3y-1TNdqQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xGv2wu1MyAPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "For the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\n",
        "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ],
      "metadata": {
        "id": "MmNi93C-4rLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ],
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use a CNN layer with kernel size = patch_size and stride = patch_size\n",
        "- Output is a sequence of linear embeddings for each patch - reshape"
      ],
      "metadata": {
        "id": "Llqd_wfn8iPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "      super().__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.image_size = image_size \n",
        "      self.patch_size = patch_size\n",
        "      self.in_channels = in_channels\n",
        "\n",
        "      self.cnn = nn.Sequential(nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)) # Takes each patch and converts it into embedding dimension\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      self.batch_size = x.shape[0]\n",
        "      x = self.cnn(x) # Shape is batch_size * embed_dim * num_patches_w * num_patches_h\n",
        "      x.permute(0, 2, 3, 1) # batch_size * __ * __ * embed_dim -> Moved embedding dimension towards the end\n",
        "      return x.view(self.batch_size, -1, self.embed_dim) # batch_size * num_patches * embed_dim [equivalent of batch_size * sequence_length * embed_dim]"
      ],
      "metadata": {
        "id": "rAzsdK5YybDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ],
      "metadata": {
        "id": "1mk8v66y6MAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Multiple attention heads independently compute scaled dot-product attention on input embeddings \n",
        "- Individual heads are concatenated and linearly transformed to the original embedding dimension\n",
        "- Calculate attention also here\n",
        "- matmul(Q, K)\n",
        "- softmax(in the embedding dimension)\n",
        "- matmul V with softmax"
      ],
      "metadata": {
        "id": "jrbHHx50B3ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "      super().__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.num_heads = num_heads\n",
        "      # weight matrices\n",
        "      self.wQ = nn.Linear(embed_dim, embed_dim) #Typically kept equal to the embedding dimension\n",
        "      self.wK = nn.Linear(embed_dim, embed_dim) #Typically kept equal to the embedding dimension\n",
        "      self.wV = nn.Linear(embed_dim, embed_dim) #Typically kept equal to the embedding dimension\n",
        "      # Remeber that the weights of all the attention heads are initialized to the same value\n",
        "      ## Hence, the linear transformation on the input should occur first and then the splitting of the attention heads - Correct\n",
        "      ### Why is W_q embed_dim * embed_dim when the input is batch_size * num_patches * embed_dim\n",
        "      ### This means that each patch is also supplied the same weight? - Yes - Improves generalization and the dimension is embedding * embedding to keep dimensionality consistetly equal to batch_size * num_patches * embed_dim \n",
        "\n",
        "      self.wH = nn.Linear(embed_dim, embed_dim) \n",
        "\n",
        "\n",
        "    def split_heads(self, x, num_heads):\n",
        "      self.num_heads = num_heads\n",
        "      self.rf_size = int(x.shape[2]/num_heads) # each attention head works on 512/8(for eight heads) = 64 of the embedding dimension\n",
        "     # rf_size -> RF stands for receptive field\n",
        "      return x.view(x.shape[0], -1, num_heads, self.rf_size).transpose(1, 2) # (batch_size, num_heads, embedding_dimension, RF_size)\n",
        "\n",
        "    def group_heads(self, x):\n",
        "      # Takes (batch_size, num_heads, RF_size, embedding_dimension) and concatenates it to batch_size\n",
        "      # (batch_size, num_heads, embedding_dimension, RF_size) -> # (batch_size, embedding_dimension, num_heads, RF_size)\n",
        "      return x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.rf_size) # Contiguous is done to ensure that memory takes up contiguous positions\n",
        "      \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      ## Your image has been broken down into patches \n",
        "      ## Break down into multiple attention heads = self.num_heads\n",
        "\n",
        "      Q = self.split_heads(self.wQ(x), self.num_heads) # batch_size * num_patches * embed_dim -> Shared weights across all tokens \n",
        "      Q = Q/np.sqrt(self.embed_dim) # Same scaling factor as Language transformer \n",
        "      K = self.split_heads(self.wK(x), self.num_heads) \n",
        "      V = self.split_heads(self.wV(x), self.num_heads)\n",
        "      A = torch.matmul(Q, K.transpose(-2, -1)) # If there are more than 2 dimensions, PyTorch identifies the first dimsnion as the batch size and handles it \n",
        "      # So A is batch_size * num_patches * num_patches\n",
        "      # Now for each of the patches, we will take a softmax\n",
        "      A = torch.softmax(A, dim = -1) #Impute normalizing constant -> Done in Q\n",
        "      # Now we have to group the heads together\n",
        "      H = torch.matmul(A, V) # batch_size * num_patches * num_patches and # batch_size * num_patches * embed_dim -> batch_size * num_patches * embed_dim (embed_dim/num_heads)\n",
        "      H_cat = self.group_heads(H) # batch_size * num_patches * embed_dim\n",
        "      H_cat = self.wH(H_cat) # Ensure dimensional consistency -> Again only scales the last dimension\n",
        "      return H_cat, A"
      ],
      "metadata": {
        "id": "V1LeAZq-0dQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "You may also want to use layer normalization or other type of normalization."
      ],
      "metadata": {
        "id": "NCAURJGJ6jhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement GeLU\n",
        "class GeLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "d51ZjCB6tmmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "id": "QvM9uXfh314l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.k1convL1 = nn.Linear(d_model,    hidden_dim)\n",
        "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.activation = GeLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.k1convL1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.k1convL2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8oiD8R5iLim0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "      super().__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.mlp_dim = mlp_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.dropout = dropout\n",
        "      self.mha = MultiHeadSelfAttention(embed_dim, num_heads) # Passing x to forward \n",
        "      self.cnn = CNN(embed_dim, mlp_dim)\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "      ## Batch norm layer\n",
        "      self.ln1 = nn.LayerNorm(embed_dim, eps=1e-6) # Small value added to the denominator to avoid division by 0\n",
        "      ## MLP layer\n",
        "\n",
        "      ## Batch norm layer\n",
        "      self.ln2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "      ## where to dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "      ## Residual connections\n",
        "        h, _ = self.mha(x)\n",
        "        h = self.dropout1(h) # Dropout after MHA\n",
        "        x = self.ln1(x + h) # Residual + MHA passed to LayerNorm\n",
        "        fcn_x = self.cnn(x)\n",
        "        fcn_x = self.dropout2(fcn_x)\n",
        "        x = self.ln2(x + fcn_x)\n",
        "        return x\n",
        "         "
      ],
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VisionTransformer:\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ],
      "metadata": {
        "id": "rgLfJRUm7EDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not the right way to implement positional embeddings; ViT uses learnable positional embeddings \n",
        "# ten = np.array([np.zeros(512) for i in range(196)])\n",
        "# nums = np.array([i for i in range(512)])\n",
        "# for i in range(196):\n",
        "#   ten[i, nums[i]] = 1"
      ],
      "metadata": {
        "id": "HodM2mf-dWg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.max_embedding = image_size/patch_size\n",
        "        self.embeddings = PatchEmbedding(image_size, patch_size, in_channels, embed_dim) # Creates embeddings\n",
        "        ## Add positional embeddings \n",
        "        self.pos_embeddings = nn.Embedding(self.max_embedding + 1, embed_dim) # +1 for the CLS token\n",
        "\n",
        "        ## Some number of transformer layers\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "          self.enc_layers.append(TransformerBlock(embed_dim, num_heads, mlp_dim, dropout))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embeddings(x)\n",
        "        # Add the CLS token \n",
        "\n",
        "        # Initialize embeddings randomly \n",
        "        pos_embeddings = self.pos_embeddings()\n",
        "        x = embeddings + pos_embeddings\n",
        "\n",
        "        # Run through the ViT blocks\n",
        "        for i in range(self.num_layers):\n",
        "          x = self.enc_layers[i](x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embeddings = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)  # Creates embeddings\n",
        "\n",
        "        # +1 for the CLS token\n",
        "        self.pos_embeddings = nn.Parameter(torch.zeros(1, (image_size // patch_size)**2 + 1, embed_dim))\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.enc_layers.append(TransformerBlock(embed_dim, num_heads, mlp_dim, dropout))\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, stochastic_depth_rate = 0.1):\n",
        "      # Implemented stochastic depth to randomly dropout layers\n",
        "        batch_size = x.shape[0]\n",
        "        embeddings = self.embeddings(x)\n",
        "\n",
        "        # Add the CLS token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embeddings\n",
        "\n",
        "        # Run through the ViT blocks\n",
        "        for i in range(self.num_layers):\n",
        "            if self.training and torch.rand(1).item()<stochastic_depth_rate:\n",
        "              continue\n",
        "            else:\n",
        "              x = self.enc_layers[i](x)\n",
        "\n",
        "        # Classifier head\n",
        "        x = x[:, 0]\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8b-JTiSgtFRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
      ],
      "metadata": {
        "id": "lROdKoO37Uqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example usage:\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 384\n",
        "num_heads = 2\n",
        "mlp_dim = 3072\n",
        "num_layers = 1\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "- 19.61%\n",
        "\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 384\n",
        "num_heads = 2\n",
        "mlp_dim = 3072\n",
        "num_layers = 1\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "- 17.92%\n",
        "\n",
        "image_size = 224\n",
        "in_channels = 3\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "patch_size = 16\n",
        "embed_dim = 384\n",
        "num_heads = 4\n",
        "mlp_dim = 1536\n",
        "num_layers = 1\n",
        "- BAD\n",
        "\n",
        "image_size = 32\n",
        "patch_size = 8\n",
        "in_channels = 3\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "mlp_dim = 2048\n",
        "num_layers = 12\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.01\n",
        "num_epochs = 50\n",
        "- 39%\n",
        "- num_steps_per_epoch = len(trainloader)\n",
        "T_0 = num_steps_per_epoch * 10  # Restart every 10 epochs\n",
        "T_mult = 1\n",
        "lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=1e-6)\n",
        "\n",
        "image_size = 96\n",
        "patch_size = 8\n",
        "in_channels = 3\n",
        "embed_dim = 384\n",
        "num_heads = 6\n",
        "mlp_dim = 1536\n",
        "num_layers = 1\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0001\n",
        "num_epochs = 70\n",
        "- Overfit"
      ],
      "metadata": {
        "id": "BV9R7_KqWsQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32\n",
        "patch_size = 8 # 4 or 8\n",
        "in_channels = 3\n",
        "embed_dim = 300 # Try 512\n",
        "num_heads = 12\n",
        "mlp_dim = embed_dim*2 \n",
        "num_layers = 9 # Increase num_layers\n",
        "num_classes = 100\n",
        "dropout = 0.25\n",
        "batch_size = 256 # Lower batch size\n",
        "num_epochs = 200"
      ],
      "metadata": {
        "id": "zL6ThNZmnK5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "1V14TFbM8x4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep around 2M parameters - experimenting with approx 4M parameters\n",
        "print (sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "na-FqEblbtYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "from PIL import ImageOps\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "class AutoAugment:\n",
        "    def __call__(self, img):\n",
        "        return ImageOps.autocontrast(img)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    Lambda(lambda img: AutoAugment()(img)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "3BOp450mdC-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
        "learning_rate = 0.0002\n",
        "weight_decay = 0.00005\n",
        "optimizer = optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "num_steps_per_epoch = len(trainloader)\n",
        "T_0 = num_steps_per_epoch * 10  # Restart every 10 epochs\n",
        "T_mult = 1\n",
        "lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=1e-6)"
      ],
      "metadata": {
        "id": "0Mn6q76XdnId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim.lr_scheduler import LambdaLR\n",
        "# warmup_epochs = 10\n",
        "# base_lr = 0.0005\n",
        "# def lr_schedule(epoch):\n",
        "#     if epoch < warmup_epochs:\n",
        "#         return (batch_size / 256) * (epoch + 1) / warmup_epochs\n",
        "#     else:\n",
        "#         t = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)\n",
        "#         return (batch_size / 256) * 0.5 * (1 + math.cos(math.pi * t))\n",
        "\n",
        "# lr_scheduler = LambdaLR(optimizer, lr_schedule)"
      ],
      "metadata": {
        "id": "LdJk76g2ZTsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/DL-HW4/best_model5.pth\"\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "# # # Restore the model and optimizer states\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "b9AHwbzHDJOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- best_model1: 40% - Model starts to overfit after 20th epoch"
      ],
      "metadata": {
        "id": "kM5EAWJGstNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = num_epochs\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        running_train_loss = running_train_loss + loss.item()\n",
        "        correct += (predicted==labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_acc = 100*correct/total\n",
        "    print(f\"Epoch: {epoch + 1}, Training Accuracy: {train_acc:.2f}%\")\n",
        "    avg_train_loss = running_train_loss / len(trainloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_losses.append(running_val_loss/len(testloader))\n",
        "    val_acc = 100 * correct / total\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoint_path)"
      ],
      "metadata": {
        "id": "eOyk345ve5HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ERA_ISPXU4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(val_accuracies)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S8dAEm-aeZn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit your best_model.pth with this notebook. And report the best test results you get."
      ],
      "metadata": {
        "id": "-AfNVj1U9xhk"
      }
    }
  ]
}